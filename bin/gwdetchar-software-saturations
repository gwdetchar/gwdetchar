#!/usr/bin/env python
# coding=utf-8
# Copyright (C) LIGO Scientific Collaboration (2015-)
#
# This file is part of the GW DetChar python package.
#
# GW DetChar is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# GW DetChar is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with GW DetChar.  If not, see <http://www.gnu.org/licenses/>.

"""Find channels clipping their software saturation limits
"""

from __future__ import print_function

import argparse
import itertools
import os
import re
import subprocess
import sys
from multiprocessing import cpu_count

import lalframe

from glue import markup

from gwpy.segments import (Segment, SegmentList,
                           DataQualityFlag, DataQualityDict)
from gwpy.time import to_gps
from gwpy.timeseries import (TimeSeries, TimeSeriesDict, StateTimeSeries)

from gwdetchar import (version, cli, const)
from gwdetchar.io.datafind import find_frames
from gwdetchar.saturation import find_saturations

__author__ = 'Dan Hoak <daniel.hoak@ligo.org>'
__credits__ = 'Duncan Macleod <duncan.macleod@ligo.org>'
__version__ = version.version

DEFAULT_NPROC = min(8, cpu_count())

re_limit = re.compile('_LIMIT\Z')
re_limen = re.compile('_LIMEN\Z')
re_swstat = re.compile('_SWSTAT\Z')
re_software = re.compile(
    '(%s)' % '|'.join([re_limit.pattern, re_limen.pattern, re_swstat.pattern]))


# -- utilities ----------------------------------------------------------------

def grouper(iterable, n, fillvalue=None):
    """Separate an iterable into sub-sets of `n` elements
    """
    args = [iter(iterable)] * n
    return itertools.izip_longest(fillvalue=fillvalue, *args)


def get_adc_channels(framefile):
    """Return the names of all ADC channels stored in the framefile
    """
    # try just using FrChannels in a subprocess, it's faster
    try:
        frchans = subprocess.Popen(
            ['FrChannels', framefile], stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)
        out, err = frchans.communicate()
        if frchans.returncode:
            raise subprocess.CalledProcessError("FrChannels %s" % framefile)
    # otherwise manually open the frame and read the ADCs
    except (OSError, subprocess.CalledProcessError):
        frfile = lalframe.FrameUFrFileOpen(framefile, "r")
        frtoc = lalframe.FrameUFrTOCRead(frfile)
        nadc = lalframe.FrameUFrTOCQueryAdcN(frtoc)
        return [lalframe.FrameUFrTOCQueryAdcName(frtoc, i) for
                i in range(nadc)]
    else:
        return zip(*map(str.split, out.splitlines()))[0]


def find_limit_channels(channels, skip=None):
    """Find all 'LIMIT' channels that have a matching 'LIMEN' or 'SWSTAT'

    Parameters
    ----------
    channels : `list` of `str`
        the list of channel names to search

    Returns
    -------
    limits : `list` or `str`
        the list of channels whose name ends in '_LIMIT' for whom a matching
        channel ending in '_LIMEN' or '_SWSTAT' was found
    """
    # find relevant channels and sort them
    if skip:
        re_skip = re.compile('(%s)' % '|'.join(skip))
        useful = sorted(x for x in channels if re_software.search(x) and
                        not re_skip.search(x))
    else:
        useful = sorted(x for x in channels if re_software.search(x))

    # map limits to limen or swstat
    limens = []
    swstats = []
    for i, x in enumerate(useful):
        if not re_limit.search(x):
            continue
        if re_limen.search(useful[i-1]):
            limens.append(x[:-6])
        elif re_swstat.search(useful[i+1]):
            swstats.append(x[:-6])
    return limens, swstats


def is_saturated(channel, cache, start=None, end=None,
                 indicator='LIMEN', nproc=DEFAULT_NPROC, segments=True):
    """Check whether a channel has saturated its software limit

    Parameters
    ----------
    channel : `str`, or `list` of `str`
        either a single channel name, or a list of channel names
    cache : `~glue.lal.Cache`
        a `~glue.lal.Cache` of file paths, the cache must be contiguous
    start : `~gwpy.time.LIGOTimeGPS`, `int`
        the GPS start time of the check
    end : `~gwpy.time.LIGOTimeGPS`, `int`
        the GPS end time of the check
    indicator : `str`
        the suffix of the indicator channel, either `'LIMEN'` or `'SWSTAT'`
    nproc : `int`
        the number of parallel processes to use for frame reading
    segments : `bool`, default `True`
        if `True` return the actual saturation segments, otherwise just
        return `True` is the channel saturates at least once

    Returns
    -------
    saturated : `bool`, `None`, or `DataQualityFlag`, or `list` of the same
        one of the following given the conditions

        - `None` : if the channel doesn't have a software limit
        - `False` : if the channel didn't saturate
        - `True` : if the channel did saturate and `segments=False` was given
        - `~gwpy.segments.DataQualityFlag` : otherwise

        OR, a `list` of the above if a `list` of channels was given in the
        first place
    """
    if isinstance(channel, (list, tuple)):
        channels = channel
    else:
        channels = [channel]
    # parse prefix
    for i, c in enumerate(channels):
        if c.endswith('_LIMIT'):
            channels[i] = c[:-6]
    # check limit if set
    indicators = ['%s_%s' % (c, indicator) for c in channels]
    data = TimeSeriesDict.read(
        cache[0].path, indicators, start=start, end=start+1, format='framecpp',
        type='adc')
    if indicator.upper() == 'LIMEN':
        active = dict((c, data[indicators[i]].value[0]) for
                      i, c in enumerate(channels))
    elif indicator.upper() == 'SWSTAT':
        active = dict(
            (c, data[indicators[i]].astype('uint32').value[0] >> 13 & 1) for
            i, c in enumerate(channels))
    else:
        raise ValueError("Don't know how to determine if limit is set for "
                         "indicator %r" % indicator)
    # get output/limit data for all with active limits
    activechans = [c for c in channels if active[c]]
    datachans = ['%s_%s' % (c, s) for c in activechans for
                 s in ('LIMIT', 'OUTPUT')]
    data = TimeSeriesDict.read(cache, datachans, start=start, end=end,
                               nproc=nproc, type='adc')
    # loop over channels and return the following:
    #     no limit      -   None
    #     no saturation -   False
    #     saturation    -   True
    out = []
    for c in channels:
        if c not in activechans:
            out.append(None)
            continue
        if segments:
            out.append(find_saturations(data['%s_OUTPUT' % c],
                                        data['%s_LIMIT' % c], precision=.99,
                                        segments=True))
        else:
            out.append(find_saturations(data['%s_OUTPUT' % c],
                                        data['%s_LIMIT' % c],
                                        precision=.99).any())
    if isinstance(channel, (list, tuple)):
        return out
    else:
        return out[0]


# -----------------------------------------------------------------------------
#
# Execution starts here
#
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# parse command line

parser = argparse.ArgumentParser()
cli.add_gps_start_stop_arguments(parser)
cli.add_ifo_option(parser)
cli.add_frametype_option(parser, required=const.IFO is None,
                         default=const.IFO is not None and '%s_R' % const.IFO)
cli.add_nproc_option(parser)
parser.add_argument('-c', '--channels',
                    help="file containing columnar list of channels to "
                         "process, default is to find all relevant channels "
                         "from frames")
parser.add_argument('-s', '--skip', action='append', default=[],
                    help='skip channels matching this string')
parser.add_argument('-g', '--group-size', default=256, type=int,
                    help="number of channels to process in a single batch, "
                         "default: %(default)s")
parser.add_argument('-a', '--state-flag', metavar='FLAG',
                    help='restrict search to times when FLAG was active')
parser.add_argument('-m', '--html', help='path to write html output')

args = parser.parse_args()

ifo = args.ifo.upper()
site = ifo[0]
frametype = args.frametype or '%s_R' % ifo

# get segments
if args.state_flag:
    state = DataQualityFlag.query(args.state_flag, args.gpsstart.seconds,
                                  args.gpsend.seconds,
                                  url=const.O1_SEGMENT_SERVER)
    segs = state.active
    print("Recovered %d seconds of time for %s"
          % (abs(segs), args.state_flag))
else:
    segs = SegmentList([Segment(args.gpsstart, args.gpsend)])

# find frames
cache = find_frames(site, frametype,
                    args.gpsstart.seconds, args.gpsend.seconds)
tcache = find_frames(site, '%s_T' % ifo, args.gpsstart.seconds,
                     args.gpsend.seconds)

# find channels
if not os.getenv('LIGO_DATAFIND_SERVER'):
    raise RuntimeError("No LIGO_DATAFIND_SERVER variable set, don't know "
                       "how to discover channels")
else:
    print("Finding channels in frames...")
    if len(cache) == 0:
        raise RuntimeError("No frames recovered for %s in interval [%s, %s)"
                           (frametype, args.gpsstart.seconds,
                            args.gpsend.seconds))
    allchannels = get_adc_channels(cache[0].path)
    print("   Found %d channels in frame" % len(allchannels))
    sys.stdout.flush()
    channels = find_limit_channels(allchannels, skip=args.skip)
    print("   Parsed %d channels with '_LIMIT' and '_LIMEN' or '_SWSTAT'"
          % sum(map(len, channels)))


# -- read channels and check limits -------------------------------------------

saturations = DataQualityDict()
bad = set()

# TODO: use multiprocessing to separate channel list into discrete chunks
#       should give a factor of X for X processes

# check limens
for suffix, clist in zip(['LIMEN', 'SWSTAT'], channels):
    nchans = len(clist)
    # group channels in sets for batch processing
    #     min of <number of channels>, user group size (sensible number), and
    #     512 Mb of RAM for single-precision EPICS
    dur = max([float(abs(s)) for s in segs])
    ngroup = int(
        min(nchans, args.group_size, .5 * 1024**3 / 4. / 16. / dur))
    print('Processing %s channels in groups of %d:' % (suffix, ngroup))
    sys.stdout.flush()
    sets = grouper(clist, ngroup)
    for i, cset in enumerate(sets):
        # remove empty entries use to pad the list to 8 elements
        cset = list(cset)
        while cset[-1] is None:
            cset.pop(-1)
        for seg in segs:
            cache2 = cache.sieve(segment=seg)
            if not len(cache2):
                continue
            saturated = is_saturated(cset, cache2, seg[0], seg[1],
                                     indicator=suffix, nproc=args.nproc,
                                     segments=True)
            for j, (c, sat) in enumerate(zip(cset, saturated)):
                if sat is not None:
                    try:
                        saturations[c] += sat
                    except KeyError:
                        saturations[c] = sat
        for j, c in enumerate(cset):
            if c in saturations:
                sat = saturations[c]
                if abs(sat.active):
                    print('%40s: ---- FAIL ---- [%d/%d]'
                          % (c, i*ngroup + j + 1, nchans))
                    for seg in sat.active:
                        print(" " * 42 + str(seg))
                    bad.add(c)
                else:
                    print('%40s:      PASS      [%d/%d]'
                          % (c, i*ngroup + j + 1, nchans))
            else:
                print('%40s:      SKIP      [%d/%d]'
                      % (c, i*ngroup + j + 1, nchans), end='\r')
            sys.stdout.flush()

# -----------------------------------------------------------------------------
# print results and exit

print("\n----------------------------------------------------------------"
      "-----")
if len(bad):
    print("Saturations were found for all of the following:")
else:
    print("No software saturations were found in any channels")
print("---------------------------------------------------------------------")
for c in bad:
    print(c)
if len(bad):
    print("-------------------------------------------------------------------"
          "--")

# write LIGO_LW XML
outfile = ('%s-SOFTWARE_SATURATIONS-%d-%d.xml.gz'
           % (ifo, args.gpsstart.seconds,
              args.gpsend.seconds - args.gpsstart.seconds))
saturations.write(outfile)
print("Saturation segments written to \n%s" % outfile)

if args.html:
    page = markup.page()
    if os.path.basename(args.html) == 'index.html':
        page.init(title='%s software saturations' % ifo,
                  css="//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/"
                      "bootstrap.min.css",
                  script=["https://code.jquery.com/jquery-1.11.2.min.js",
                          "//maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/"
                          "bootstrap.min.js"])
    page.div(class_='container')
    page.div(class_='page-header')
    page.h1('%s software saturations: %d-%d'
            % (ifo, args.gpsstart.seconds, args.gpsend.seconds))
    if args.state_flag:
        page.p()
        page.strong("State flag: ")
        page.add(args.state_flag)
        page.p.close()
    page.div.close()
    page.table(class_='table table-striped table-hover')
    # write table header
    page.thead()
    page.tr()
    for header in ['Channel', 'Result', 'Num. saturations']:
        page.th(header)
    page.thead.close()
    # write body
    page.tbody()
    for c, seglist in saturations.iteritems():
        passed = abs(seglist.active) == 0
        page.tr(class_=passed and 'default' or 'danger')
        page.td(c)
        page.td(passed and 'Pass' or 'Fail')
        page.td(len(seglist.active))
        page.tr.close()
    page.tbody.close()
    page.table.close()
    page.div.close()

    with open(args.html, 'w') as fp:
        print(str(page), file=fp)
