#!/usr/bin/env python
# coding=utf-8
# Copyright (C) LIGO Scientific Collaboration (2015-)
#
# This file is part of the GW DetChar python package.
#
# GW DetChar is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# GW DetChar is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with GW DetChar.  If not, see <http://www.gnu.org/licenses/>.

"""Batch-generate a series of Omega-pipeline scans.

GPS times can be given individually on the command-line, one after the other,
or can be bundled into one file formatted where the first column contains
the GPS times (other columns are ignored).

The output of this script is a condor workflow in the form of a DAG file,
with associated condor submit (`.sub`) file and equivalent shell script (`.sh`)
in the output directory.
Submitting the workflow to Condor will result in the scans being processed
in parallel, or you can just run the `.sh` script to process in serial.
"""

import os
from getpass import getuser

from glue import pipeline

from gwdetchar import (omega, cli, condor)

# attempt to get WDQ path
WDQ = os.path.join(os.path.dirname(__file__), 'wdq')
if not os.path.isfile(WDQ):
    WDQ = None

# set default accounting information
CONDOR_ACCOUNTING_GROUP = os.getenv(
    '_CONDOR_ACCOUNTING_GROUP', 'ligo.dev.{epoch}.detchar.user_req.omegascan')
CONDOR_ACCOUNTING_USER = os.getenv(
    '_CONDOR_ACCOUNTING_USER', getuser())


# -- parse command line -------------------------------------------------------

parser = cli.create_parser(description=__doc__)

parser.add_argument('gps-time', nargs='+',
                    help='GPS time(s) to scan, or path to a file '
                         'containing a single column of such times')
cli.add_ifo_option(parser)
parser.add_argument('-o', '--output-dir', default=os.getcwd(),
                    help='output directory for all scans')

parser.add_argument(
    '-f', '--config-file',
    help='path to configuration file to use, default: '
         'choose based on observatory, epoch, and pipeline')
parser.add_argument('-q', '--wdq', default=WDQ, required=WDQ is None,
                    help='path to wdq executable')
parser.add_argument('-w', '--wpipeline', default='gwdetchar-omega',
                    nargs='?', const=omega.WPIPELINE,
                    help='path to wpipeline binary (default: %(default)s), '
                         'can choose the Matlab version by passing this flag '
                         'with no argument')
parser.add_argument('--colormap', default=None,
                    help='name of colormap to use (supported either in Python '
                         'or in Matlab omega > r3449), default: choose based '
                         'on wpipeline')

margs = parser.add_argument_group('Matlab options')
margs.add_argument(
    '-c', '--cache-file',
    help='path to data cache file, if not given, data locations '
         'are found using the datafind server, must be in LAL cache format')

pargs = parser.add_argument_group('Python options')
pargs.add_argument('-d', '--disable-correlation', action='store_true',
                   default=False, help='disable cross-correlation of aux '
                                       'channels, default: False')
pargs.add_argument('-t', '--far-threshold', type=float, default=1e-10,
                   help='white noise false alarm rate threshold for '
                        'processing channels, default: %(default)s Hz')
pargs.add_argument('-s', '--ignore-state-flags', action='store_true',
                   default=False, help='ignore state flag definitions in '
                                       'the configuration, default: False')
cli.add_nproc_option(pargs)
pargs.add_argument('-v', '--verbose', action='store_true', default=False,
                   help='print verbose output, default: False')

cargs = parser.add_argument_group('Condor options')
cargs.add_argument('-u', '--universe', default='vanilla', type=str,
                   help='universe for condor processing')
cargs.add_argument('--condor-accounting-group',
                   default=CONDOR_ACCOUNTING_GROUP,
                   help='accounting_group for condor submission on the LIGO '
                        'Data Grid, include \'{epoch}\' (with curly brackets) '
                        'to auto-substitute the appropriate epoch based on '
                        'the GPS times')
cargs.add_argument('--condor-accounting-group-user',
                   default=CONDOR_ACCOUNTING_USER,
                   help='accounting_group_user for condor submission on the '
                        'LIGO Data Grid')
cargs.add_argument('--condor-timeout', type=float, default=None, metavar='T',
                   help='configure condor to terminate jobs after T hours '
                        'to prevent idling, default: %(default)s')
cargs.add_argument('--condor-command', action='append', default=[],
                   help="Extra condor submit commands to add to "
                        "gw_summary submit file. Can be given "
                        "multiple times in the form \"key=value\"")

args = parser.parse_args()

outdir = os.path.abspath(os.path.expanduser(args.output_dir))

# parse times
times = getattr(args, 'gps-time')

if len(times) == 1:
    try:  # try converting to GPS
        times = [float(times[0])]
    except (TypeError, ValueError):  # otherwise read as file
        import numpy
        times = numpy.loadtxt(times[0], dtype=float, ndmin=1)
else:
    times = list(map(float, times))

# finalise accounting tag based on run
if '{epoch}' in args.condor_accounting_group:
    gpsepoch = max(times)
    epoch = condor.accounting_epoch(gpsepoch)
    args.condor_accounting_group = args.condor_accounting_group.format(
        epoch=epoch.lower())

# valid the accounting tag up-front
try:
    valid = condor.validate_accounting_tag(args.condor_accounting_group)
except EnvironmentError:
    valid = True  # failed to load condor tags, not important
if not valid:
    listtags = 'cat {0} | json_pp | less'.format(condor.ACCOUNTING_GROUPS_FILE)
    raise ValueError("condor accounting tag {0!r} recognised, to see the list "
                     "of valid groups, run `{1}`".format(
                         args.condor_accounting_group, listtags))

# set colormap default
if not args.colormap and args.wpipeline.endswith('wpipeline'):
    args.colormap = 'parula'  # matlab
elif not args.colormap:
    args.colormap = 'viridis'  # python

# -- generate workflow --------------------------------------------------------

tag = 'wdq-batch'

# generate directories
logdir = os.path.join(outdir, 'logs')
subdir = os.path.join(outdir, 'condor')
for d in [outdir, logdir, subdir]:
    if not os.path.isdir(d):
        os.makedirs(d)

# start workflow
dag = pipeline.CondorDAG(os.path.join(logdir, '%s.log' % tag))
dag.set_dag_file(os.path.join(subdir, tag))
dagfile = dag.get_dag_file()

# configure wdq job
job = pipeline.CondorDAGJob(args.universe, args.wdq)
job.set_sub_file('%s.sub' % os.path.splitext(dagfile)[0])
logstub = os.path.join(logdir, '%s-$(cluster)-$(process)' % tag)
job.set_log_file('%s.log' % logstub)
job.set_stdout_file('%s.out' % logstub)
job.set_stderr_file('%s.err' % logstub)

# add custom condor commands, using defaults
condorcmds = {
    'getenv': True,
    'accounting_group': args.condor_accounting_group,
    'accounting_group_user': args.condor_accounting_group_user,
}
if args.universe != 'local':
    condorcmds['request_memory'] = 4096
if args.condor_timeout:
    condorcmds['periodic_remove'] = (
        'CurrentTime-EnteredCurrentStatus > %d' % (3600 * args.condor_timeout))
for cmd_ in args.condor_command:
    key, value = cmd_.split('=', 1)
    condorcmds[key.rstrip().lower()] = value.strip()
for key, val in condorcmds.items():
    job.add_condor_cmd(key, val)

# add common wdq options
job.add_opt('wpipeline', args.wpipeline)
job.add_opt('colormap', args.colormap)
job.add_opt('ifo', args.ifo)
if args.config_file is not None:
    job.add_opt('config-file', os.path.abspath(args.config_file))
if args.wpipeline.endswith('wpipeline'):
    job.add_opt('condor', '')
    if args.cache_file is not None:
        job.add_opt('cache-file', args.cache_file)
else:
    job.add_opt('far-threshold', str(args.far_threshold))
    job.add_opt('nproc', str(args.nproc))
    if args.disable_correlation:
        job.add_opt('disable-correlation', '')
    if args.ignore_state_flags:
        job.add_opt('ignore-state-flags', '')
    if args.verbose:
        job.add_opt('verbose', '')

# make node in workflow for each time
for t in times:
    node = pipeline.CondorDAGNode(job)
    node.set_category('wdq')
    node.set_retry(1)
    node.add_var_arg(str(t))
    node.add_var_opt('output-dir', os.path.join(outdir, str(t)))
    dag.add_node(node)

# write DAG
dag.write_sub_files()
dag.write_dag()
dag.write_script()

# print instructions for the user
shfile = '%s.sh' % os.path.splitext(dagfile)[0]
print("Workflow generated for %d times" % len(times))
print("Run in the current shell via:\n\n$ %s\n" % shfile)
if os.path.isfile('%s.rescue001' % dagfile):
    print("Or, submit to condor via:\n\n$ condor_submit_dag -force %s"
          % dagfile)
else:
    print("Or, submit to condor via:\n\n$ condor_submit_dag %s" % dagfile)
